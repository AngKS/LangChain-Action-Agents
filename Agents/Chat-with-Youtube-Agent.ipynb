{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for interacting with Youtube Video Transcription API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    temperature= 0.5,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key= os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings(\n",
    "    model=\"embed-english-light-v3.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transcript df\n",
    "def create_transcript_df(yt_transcript: list, yt_id: str):\n",
    "    return (\n",
    "        pd.DataFrame(yt_transcript)\n",
    "        .assign(start_dt=lambda x: pd.to_datetime(x[\"start\"], unit=\"s\"))\n",
    "        .set_index(\"start_dt\")\n",
    "        .resample(\"3min\")\n",
    "        .agg({\"text\": \" \".join})\n",
    "        .reset_index()\n",
    "        .assign(start_dt=lambda x: x[\"start_dt\"].dt.minute * 60)\n",
    "        .assign(\n",
    "            source=lambda x: \"https://youtu.be/\"\n",
    "            + yt_id\n",
    "            + \"&t=\"\n",
    "            + x[\"start_dt\"].astype(\"str\")\n",
    "        )\n",
    "        .drop(columns=[\"start_dt\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88307cfae74f4237824c58045287431f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching transcription:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yt_ids = [\n",
    "    \"OtD8wVaFm6E\",  # XGBoost Part 1 (of 4): Regression\n",
    "    \"8b1JEDvenQU\",  # XGBoost Part 2 (of 4): Classification\n",
    "    \"ZVFeW798-2I\",  # XGBoost Part 3 (of 4): Mathematical Details\n",
    "    \"oRrKeUCEbq8\",  # XGBoost Part 4 (of 4): Crazy Cool Optimizations\n",
    "]\n",
    "transcript_dfs = []\n",
    "for yt_id in tqdm(yt_ids, desc=\"Fetching transcription\"):\n",
    "    yt_transcript = YouTubeTranscriptApi.get_transcript(yt_id)\n",
    "    transcript_dfs.append(create_transcript_df(yt_transcript, yt_id))\n",
    "\n",
    "transcripts_df = pd.concat(transcript_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b07a839618444f4a394e18339e3ea08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split https://youtu.be/OtD8wVaFm6E&t=0 into 2 chunks\n",
      "Split https://youtu.be/OtD8wVaFm6E&t=180 into 2 chunks\n",
      "Split https://youtu.be/OtD8wVaFm6E&t=360 into 2 chunks\n",
      "Split https://youtu.be/OtD8wVaFm6E&t=540 into 2 chunks\n",
      "Split https://youtu.be/OtD8wVaFm6E&t=720 into 2 chunks\n",
      "Split https://youtu.be/OtD8wVaFm6E&t=900 into 2 chunks\n",
      "Split https://youtu.be/OtD8wVaFm6E&t=1080 into 2 chunks\n",
      "Split https://youtu.be/OtD8wVaFm6E&t=1260 into 2 chunks\n",
      "Split https://youtu.be/OtD8wVaFm6E&t=1440 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=0 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=180 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=360 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=540 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=720 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=900 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=1080 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=1260 into 2 chunks\n",
      "Split https://youtu.be/8b1JEDvenQU&t=1440 into 1 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=0 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=180 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=360 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=540 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=720 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=900 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=1080 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=1260 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=1440 into 2 chunks\n",
      "Split https://youtu.be/ZVFeW798-2I&t=1620 into 1 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=0 into 2 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=180 into 2 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=360 into 2 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=540 into 2 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=720 into 2 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=900 into 2 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=1080 into 2 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=1260 into 2 chunks\n",
      "Split https://youtu.be/oRrKeUCEbq8&t=1440 into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\" \", chunk_size=1200, chunk_overlap=150)\n",
    "\n",
    "yt_docs, yt_meta = [], []\n",
    "\n",
    "for index, row in tqdm(transcripts_df.iterrows(), total=len(transcripts_df)):\n",
    "    splits = text_splitter.split_text(row[\"text\"])\n",
    "    yt_docs.extend(splits)\n",
    "    yt_meta.extend([{\"source\": row[\"source\"]}] * len(splits))\n",
    "    print(f\"Split {row['source']} into {len(splits)} chunks\")\n",
    "\n",
    "yt_store = FAISS.from_texts(yt_docs, embeddings, metadatas=yt_meta)\n",
    "\n",
    "assert len(yt_docs) == len(yt_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "    Given the following extracted parts of a long document and a question,\n",
    "    create a final answer.\n",
    "    {context}\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    Chatbot:\"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\", \"context\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=yt_store.as_retriever(k=3),\n",
    "    memory=memory,\n",
    "    question_prompt=question_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'FINAL ANSWER: The difference in building a tree for a regression case compared to a classification case lies in the calculation of the quality score or similarity score, loss function, and the type of problem being solved. In regression, the similarity score is calculated as the sum of the residuals squared over the number of residuals plus lambda (a regularization parameter), whereas in classification, the quality score or similarity score would likely be calculated differently, possibly using metrics such as Gini impurity or information gain. Additionally, the denominator in the calculation differs between regression and classification cases, with classification involving previously predicted probabilities and regression involving previously predicted values or residuals.\\n\\n',\n",
       " 'sources': 'https://youtu.be/OtD8wVaFm6E&t=180, https://youtu.be/8b1JEDvenQU&t=0, https://youtu.be/8b1JEDvenQU&t=1260, https://youtu.be/ZVFeW798-2I&t=180'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use here either yt_ts_store or ys_store depending if you like to use source with or without timestamps\n",
    "\n",
    "result = chain(\n",
    "    {\n",
    "        \"question\": \"What is the difference in building a tree for a regression case compared to a classification case?\"\n",
    "    },\n",
    "    return_only_outputs=True,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
